---
title: "Milestone: 80-Billion-Parameter Brain Model Trained on Marlowe"
date: 2026-02-07
type: research-milestone
---

Professor Dan Yamins' team completed a landmark proof-of-concept run on Marlowe, training an 80-billion-parameter brain-inspired neural network across 24 nodes (192 H100 GPUs) for 24 hours. The run achieved 45% model FLOPS utilization (MFU) â€” validating that capability-scale computational neuroscience training is possible on Stanford's GPU supercomputer.

The Counterfactual World Model is designed to study how biological neural circuits give rise to cognitive function, and represents one of the largest neuroscience training runs ever conducted at a university.

[Read more about this research]({{ '/research/yamins/' | relative_url }})
